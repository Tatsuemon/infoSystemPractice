{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text recognition - テキスト認識\n",
    "\n",
    "The next ready solution for image data that we will test is for detecting text. This can be used for both printed and handwritten text.\n",
    "\n",
    "次に試す画像データ用のサービスはテキストの検出です。印刷した文章や手書きテキストに応用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start again by defining the environment variable for Google cloud authentication.\n",
    "\n",
    "Googleクラウド認証用の環境変数を定義することから始めましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=gcp_credentials/mlpractice2020.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next load libraries that we will need.\n",
    "\n",
    "次に必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "from google.cloud import vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition of handwritten text / 手書きテキストの認識\n",
    "\n",
    "We will first try recognizing text that we ourselves manually write. \n",
    "\n",
    "We will use the canvas input that you already saw in previous sessions when we worked with the MNIST dataset. At that time we manually implemented the learning for detecting handwritten digits 0 to 9. Compared to that the service that GCP provides is better in many aspects:\n",
    "\n",
    "- It can recognize letters, not just numbers. It also supports different languages and writting system (including kanji and japanese).\n",
    "\n",
    "- It can detect whole words and sentences, instead of just one character.\n",
    "\n",
    "- It uses state of the art machine learning solutions so it gives very good results out of the box (i.e. no need to adjust parameters etc.).\n",
    "\n",
    "まずは、自分で書いた手書きテキストの認識してみましょう。\n",
    "\n",
    "先日のMNISTデータセットのセッションですでに見たキャンバス入力ツールを使用します。当時、手書きの数字0から9を検出するための学習を手動で実装しました。それに比べて、GCPが提供するサービスは多くの点で優れています。\n",
    "\n",
    "- 数字だけでなく文字も認識できます。それはまた異なった言語や文字をサポートしている（漢字・日本語も使えます）\n",
    "\n",
    "- 1文字だけでなく、単語や文全体を検出できる\n",
    "\n",
    "- 最先端の機械学習手法を使用しているので、そのままで（つまり、学習やパラメータ調整を何もしなくても）非常にいい結果が得られる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to setup the canvas for writing. You do not need to understand the details of the code. It is enough just to know that every time something is written on the canvas the canvas content will be automatically saved inside a variable named `image` (as numpy array).\n",
    "\n",
    "以下は書き込み用のキャンバスを設定するためのコードです。このコードの詳細を理解する必要はありません。キャンバスに何かが書かれるたびに、キャンバスの中身が`image`というnumpy配列に保存される、という仕組みになっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import jupyter_drawing_pad as jd\n",
    "\n",
    "out1 = widgets.Output(layout=widgets.Layout(width='400px'))\n",
    "jdp = jd.CustomBox()\n",
    "\n",
    "draw_pad = jdp.drawing_pad\n",
    "clear_btn = jdp.children[1].children[1]\n",
    "\n",
    "@out1.capture() \n",
    "def w1_CB(change):\n",
    "    from scipy.signal import convolve2d\n",
    "    from cv2 import resize, INTER_CUBIC\n",
    "    \n",
    "    global image\n",
    "\n",
    "    data = change['new']\n",
    "    if len(data[0]) > 2:\n",
    "        # Format the canvas output to obtain a 28 x 28 image \n",
    "        x = np.array(data[0])\n",
    "        y = np.array(data[1])\n",
    "        \n",
    "        # we plot stroke by stroke for \n",
    "        # assuming there is at least 200ms between each stroke \n",
    "        line_breaks = np.where(np.diff(np.array(data[2])) > 200)[0]\n",
    "        # adding end of array\n",
    "        line_breaks = np.append(line_breaks,len(data[2]))\n",
    "        \n",
    "        # Plot to canvas\n",
    "        from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "        fig = plt.figure()\n",
    "        canvas = FigureCanvas(fig)\n",
    "        ax = fig.gca()\n",
    "\n",
    "        # plot all strokes\n",
    "        plt.plot(x[:line_breaks[0]],y[:line_breaks[0]],color='red',linewidth=4)\n",
    "        for i in range(1,len(line_breaks)):\n",
    "            plt.plot(x[line_breaks[i-1]+1:line_breaks[i]],y[line_breaks[i-1]+1:line_breaks[i]],color='red',linewidth=4)\n",
    "        \n",
    "        plt.xlim(0,460)\n",
    "        plt.ylim(0,250)\n",
    "        plt.axis(\"off\")\n",
    "        # plt.show()    \n",
    "        \n",
    "        canvas.draw()       # draw the canvas, cache the renderer\n",
    "\n",
    "        # convert to numpy array - just for plotting\n",
    "        imageflat = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "         # not sure why this size...\n",
    "        image = np.reshape(imageflat,(288,432,3))\n",
    "        # cut figure edges\n",
    "        image = image[35:250,50:390]\n",
    "        \n",
    "        # draw again the converted image (not really necessary, but for testing)\n",
    "        plt.clf()\n",
    "        plt.imshow(image, aspect='equal', interpolation='none')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Schedule for clearing\n",
    "        out1.clear_output(wait=True)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "draw_pad.observe(w1_CB, names='data')\n",
    "\n",
    "hb = widgets.HBox([draw_pad, clear_btn, out1])\n",
    "display(hb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try writing some text in the above canvas. Japanese, English and [multiple other languages](https://cloud.google.com/vision/docs/languages) can be used - try first the one which you are most comfortable writing.\n",
    "\n",
    "上記のキャンバスにテキストを書いてみてください。日本語、英語や[その他の言語](https://cloud.google.com/vision/docs/languages)を使えます。最初は自分に最も書きやすい言語を使ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code saves the content of the canvas into a file (\"image_file.png\"), which we will use later.\n",
    "\n",
    "次のコードはキャンバスの内容をファイル\"image_file.png\"に保存します。このファイルは後で使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im = Image.fromarray(image)\n",
    "imagepath = \"image_file.png\"\n",
    "im.save(imagepath,format=\"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the image that we just saved an prepare it for sending to Google Cloud. (This part of the code is the same as we used for face detection.)\n",
    "\n",
    "それでは、保存した画像をロードして、Google Cloudに送信するための準備をしましょう。 （このコードは顔検出に使用したコードと同じです。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "with open(imagepath, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's carry out the actual detection.\n",
    "\n",
    "そして、実際の検出を実行しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = client.document_text_detection(image=image)\n",
    "\n",
    "# try using this version if you are getting wrong results and want to give a hint to the detection program which language to use\n",
    "# response = client.document_text_detection(image=image, image_context={\"language_hints\": [\"ja\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable  `response` contains the detection result. The part `full_text_annotation` gives the whole detected text.\n",
    "\n",
    "変数`response`には検出結果が含まれています。`full_text_annotation`の部分には、検出されたテキスト全体が入っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(response.full_text_annotation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the text in the canvas and running the detection again.\n",
    "\n",
    "キャンバス内のテキストを変更して、検出をもう一度実行してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition of MNIST images / MNIST画像の認識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare with results from the previous sessions let's also try to see if GCP can recognize the numbers on the MNIST dataset images.\n",
    "\n",
    "Note that now we will not have to train a neural network or some other machine learning algorithm by ourselves. We can just ask the cloud service to recognize which number is in the image.\n",
    "\n",
    "先週や先々週の結果と比較するために、GCPがMNISTデータセット画像の番号を認識できるかどうかも確認してみましょう。\n",
    "\n",
    "なお、前のセッションと違って、ニューラルネットワーク等の機械学習アルゴリズムを自分で訓練する必要は全くありません。どの数字が画像内にあるかを認識するようにクラウドサービスに依頼するだけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the cloud service performs. The following code loads 5 randomly selected images from the MNIST dataset and makes a combines them to get a single image (you will get a different image each time you execute the code).\n",
    "\n",
    "クラウドサービスがどの程度うまく機能するかを見てみましょう。 次のコードは、MNISTデータセットからランダムに選択された5つの画像をロードし、それらを組み合わせて単一の画像を取得します（コードを実行するたびに異なる画像が取得されます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_loader import MNISTImageLoader\n",
    "# initialize and randomize\n",
    "mnist_image_loader = MNISTImageLoader(np.random.randint(10000))\n",
    "# load just 1 sample\n",
    "X, y = mnist_image_loader.samples(5)\n",
    "# stack 5 images together\n",
    "X = np.hstack((X[0,:,:,0],X[1,:,:,0],X[2,:,:,0],X[3,:,:,0],X[4,:,:,0]))\n",
    "\n",
    "# plot image and print number\n",
    "plt.imshow(X) \n",
    "plt.axis('off')\n",
    "print(\"Loaded image has numbers: \" + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code saves the content of the canvas into a file (\"imageX.png\"), which we later send to the Google Cloud.\n",
    "\n",
    "次のコードは上の画像の内容をファイル\"imageX.png\"に保存します。このファイルは後でGoogleクラウドに送信します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imX = Image.fromarray(X)\n",
    "\n",
    "# the MNIST images are very small, we need to resize them to get a good detection result from GCP\n",
    "# we also invert the colors\n",
    "#imX=ImageOps.invert(imX.resize((5*140,140))) # 5x resized\n",
    "imX=ImageOps.invert(imX)\n",
    "\n",
    "imagepathX = \"imageX.png\"\n",
    "imX.save(imagepathX,format=\"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the image that we just saved an prepare it for sending to Google Cloud. (This part of the code is the same as we used for face detection.)\n",
    "\n",
    "ただいま保存した画像をロードし、Google Cloudに送信する準備をしましょう。（このコードの部分は、顔検出に使用したものと同様です。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "with open(imagepathX, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.types.Image(content=content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the actual detection.\n",
    "\n",
    "それでは、実際の検出を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = client.document_text_detection(image=image, image_context={\"language_hints\": [\"en\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable  `response` contains the detection result. The part `full_text_annotation` gives the whole detected text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(response.full_text_annotation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get the correct result?\n",
    "<br>\n",
    "(The text detection is not specialised for detecting digits, so it may not always work well.)\n",
    "\n",
    "正しい結果が出ましたか？\n",
    "<br>\n",
    "（テキスト認識は数字の認識に特化していないため、常にうまく機能するとは限りません）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Try  it yourself ! / 自分で試そう！\n",
    "\n",
    "When you are ready open the [notebook with exercises](session2-playground.ipynb).\n",
    "\n",
    "それでは、[プレイグランド](session2-playground.ipynb)を開きましょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
