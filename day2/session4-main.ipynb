{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained networks and transfer learning (very optional)\n",
    "\n",
    "This notebook is not part of the course, but you can study it if you wish. It is provided as is, without many comments.\n",
    "\n",
    "このノートブックはコースの一部ではありませんが、内容は自由に学ぶことができます。コメントが少なめ、和訳なしで提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained neural networks\n",
    "\n",
    "In this session, we will learn how to use __pretrained neural networks__. These networks were already trained on a large dataset and the weights of the trained network are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keras` library contains some pretrained models that are easy to access.\n",
    "<br>\n",
    "These models are in the `keras.applications` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try the `MobileNetV2` network that was pre-trained on the ImageNet database:\n",
    "- `MobileNetV2` is a specific architecture of CNN for image classification\n",
    "- ImageNet is a large database of images (14 million annotated images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "  458752/14536120 [..............................] - ETA: 7:30:48"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dc1247ca1cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_mobilenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/infoSystemPractice-NJP4PSLU/lib/python3.7/site-packages/tensorflow_core/python/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/infoSystemPractice-NJP4PSLU/lib/python3.7/site-packages/tensorflow_core/python/keras/applications/mobilenet_v2.py\u001b[0m in \u001b[0;36mMobileNetV2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/infoSystemPractice-NJP4PSLU/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py\u001b[0m in \u001b[0;36mMobileNetV2\u001b[0;34m(input_shape, alpha, include_top, weights, input_tensor, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mweight_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBASE_WEIGHT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             weights_path = keras_utils.get_file(\n\u001b[0;32m--> 411\u001b[0;31m                 model_name, weight_path, cache_subdir='models')\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
      "\u001b[0;32m~/.local/share/virtualenvs/infoSystemPractice-NJP4PSLU/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import the mobilenet_v2 functions\n",
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "\n",
    "# Create a pre-trained model\n",
    "model_mobilenet = mobilenet_v2.MobileNetV2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the model layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mobilenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "- There are many layers.\n",
    "- There are `3,504,872` trainable parameters.\n",
    "- The shape of the input placeholder indicates that the images must be of size `224x224` and have `3` channels; Namely `mobilenet_v2` works on color images (RGB channels).\n",
    "- The output shape of the last layer (called `Logits`) indicates that `mobilenet_v2` can recognize `1000` categories of objects. The network was trained using the data of the ImageNet Large Scale Visual Recognition Challenge that uses a subset of ImageNet with `1000` categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try `mobilenet_v2` on a few examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src='./img/ballpen.png' width='200' align=\"left\">|<img src='./img/backpack.png' width='200'>|<img src='./img/cup.png' width='200'>|<img src='./img/keyboard.png' width='200'>|\n",
    "|---|---|---|---|\n",
    "|./img/ballpen.png|./img/backpack.png|./img/cup.png|./img/keyboard.png|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.preprocessing` package provides tool for loading and formatting images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to load an image, and if necessary resize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './img/ballpen.png'\n",
    "img = image.load_img(img_path, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `image` tool loads images as a `PIL.Image.Image` object.\n",
    "<br>\n",
    "We need to transform it to a `numpy` array for the network.\n",
    "<br>\n",
    "The `PIL.Image.Image` object has a function `img_to_array` that does this transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))\n",
    "x = image.img_to_array(img)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numpy` array `x` has a shape of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mobilenet_v2` network expects an array of shape `(None, 224, 224, 3)`.\n",
    "<br>\n",
    "This means that it is a block of at least one image of size `224x224` with `3` channels.\n",
    "<br>\n",
    "Let us create a block of one image from our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.expand_dims(x, axis=0) \n",
    "# There other ways to do this conversion:\n",
    "#X = x[np.newaxis, :, :, :]\n",
    "#X = x.reshape((1,224,224,3))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras` provides a function to preprocess input images for `mobilenet_v2`.\n",
    "<br>\n",
    "The preprocessing substract a mean value (computed during training) from the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mobilenet_v2.preprocess_input(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the image is preprocessed, we can use the `mobilenet_v2` network to get the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model_mobilenet.predict(X)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the prediction is a vector of size `(1, 1000)` as we used an input of size `(None, 224, 224, 3)`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras` also provides a function to display the predictions.\n",
    "The parameter `top` limits the number of predictions to consider; here, we only access the top three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_predictions = mobilenet_v2.decode_predictions(predictions, top=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `decode_predictions` is a list containing a list of elements for each of the input images.\n",
    "<br>\n",
    "Each element has 3 fields:\n",
    "- An id from the ImageNet database for the predicted object\n",
    "- A human readable name for the object\n",
    "- The value of the output neuron corresponding to that object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gave only one input image so `decoded_predictions[0]` contains the list of element for that image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in decoded_predictions[0]:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it\n",
    "\n",
    "Check what are the predictions of mobilenet_v2 for the other 3 example images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it\n",
    "\n",
    "Process the 4 example images together in a single call to `model.predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package `keras.applications` contains many different models.\n",
    "<br>\n",
    "For example:\n",
    "- the `VGG19` model from the package `vgg19`\n",
    "- the `ResNet50` model from the package `resnet50`\n",
    "\n",
    "have the same interface as `mobilenet_v2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn to re-use a part of a trained network for solving another classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying pretrained models on our data\n",
    "\n",
    "Most of the time, the pre-trained deep networks like \"VGG19\", \"ResNet50\" and \"MobileNetV2\" are trained on a dataset (here ImageNet subset) that does not correspond to the classification task we want to do on our own data.\n",
    "<br>\n",
    "For example, if we want to create an image classifier that classify images of hard discs and ram modules, the pre-trained classifier may not be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `./data/` folder, there are two subfolders `hd/` and `ram/` each containing various images of hard discs and ram modules.\n",
    "<br>\n",
    "Let us see what kind of results `mobilenet_v2` gives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the files easily, we use the `glob` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "image_list = glob.glob('./data/hd/hd_*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can apply `mobilenet_v2` on all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for img_path in image_list:\n",
    "    try:\n",
    "        X = image.img_to_array(image.load_img(img_path, target_size=(224, 224))).reshape((1,224,224,3))\n",
    "        X = mobilenet_v2.preprocess_input(X)\n",
    "        predictions = model_mobilenet.predict(X)\n",
    "        decoded_predictions = mobilenet_v2.decode_predictions(predictions, top=1)\n",
    "        pred_list.append(decoded_predictions[0][0][1])#Just keep the readable name of the class\n",
    "    except OSError as e:\n",
    "        print(str(e))\n",
    "\n",
    "# Show one example\n",
    "plt.imshow(image.load_img(img_path, target_size=(224, 224)))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Keep one example for later use\n",
    "X_hd = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_set = set(pred_list)\n",
    "labels = []\n",
    "counts = []\n",
    "for obj in objects_set:\n",
    "    labels += [obj]\n",
    "    counts += [pred_list.count(obj)]\n",
    "\n",
    "plt.barh(2*np.arange(len(counts)),counts,1.5)\n",
    "plt.yticks(2*np.arange(len(labels)), labels, rotation='horizontal')\n",
    "plt.ylim(-1, 2*(len(counts)-0.5))\n",
    "plt.xlabel(\"counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mobilenet_v2` is able to recognize the hard disc images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mobilenet_v2 accuracy:', pred_list.count('hard_disc') / len(pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "image_list = glob.glob('./data/ram/ram_*.jpg')\n",
    "\n",
    "pred_list = []\n",
    "for img_path in image_list:\n",
    "    try:\n",
    "        X = image.img_to_array(image.load_img(img_path, target_size=(224, 224))).reshape((1,224,224,3))\n",
    "        X = mobilenet_v2.preprocess_input(X)\n",
    "        predictions = model_mobilenet.predict(X)\n",
    "        decoded_predictions = mobilenet_v2.decode_predictions(predictions, top=1)\n",
    "        pred_list.append(decoded_predictions[0][0][1])\n",
    "    except OSError as e:\n",
    "        print(str(e))\n",
    "\n",
    "# Show one example\n",
    "plt.imshow(image.load_img(img_path, target_size=(224, 224)))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Keep one example for later use\n",
    "X_ram = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_set = set(pred_list)\n",
    "labels = []\n",
    "counts = []\n",
    "for obj in objects_set:\n",
    "    labels += [obj]\n",
    "    counts += [pred_list.count(obj)]\n",
    "\n",
    "plt.barh(2*np.arange(len(counts)),counts,1.5)\n",
    "plt.yticks(2*np.arange(len(labels)), labels, rotation='horizontal')\n",
    "plt.ylim(-1, 2*(len(counts)-0.5))\n",
    "plt.xlabel(\"counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mobilenet_v2` does not know about RAM! (The reason is that RAM images are not in the ImageNet dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated pretrained network\n",
    "\n",
    "The models in the `keras.applications` package have a `include_top` parameter.\n",
    "If set to `True`, the model includes the classification part otherwise only the feature part is loaded.\n",
    "<br>\n",
    "Let us load a `mobilenet_v2` without the classification part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mobilenet_no_top = mobilenet_v2.MobileNetV2(include_top=False)\n",
    "model_mobilenet_no_top.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the full `mobilenet_v2`, last few layers are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the block size that was already `None` in the full `mobilenet_v2`, the image width and height are also set to `None` in the truncated `mobilenet_v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to an image of size `224x224` with `3` channles, the `predict` function outputs an image of size `7x7` with `1056` channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_hd = model_mobilenet_no_top.predict(X_hd)\n",
    "print(features_hd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize this image, let us concatenate all the `1056` channels in a large image.\n",
    "<br>\n",
    "We create a large image of size `(7x32)x(7x64)` by having `32` rows and `33` columns of small `7x7` images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "I_hd = np.zeros((7*32, 7 *33))\n",
    "for i in range(32):\n",
    "    for j in range(33):\n",
    "        I_hd[i*7 : (i+1)*7, j*7 : (j+1)*7] = features_hd[0,:,:,i*33+j].reshape((7,7))\n",
    "plt.imshow(I_hd)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows the representation of the input hard disc image obtained from the convolutive part of the trained `mobilenet_v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing for the ram image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ram = model_mobilenet_no_top.predict(X_ram)\n",
    "\n",
    "I_ram = np.zeros((7*32, 7 *33))\n",
    "for i in range(32):\n",
    "    for j in range(33):\n",
    "        I_ram[i*7 : (i+1)*7, j*7 : (j+1)*7] = features_ram[0,:,:,i*33+j].reshape((7,7))\n",
    "plt.imshow(I_ram)\n",
    "plt.axis(\"off\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second image is the representation of the input ram image obtained from the convolutive part of the trained `mobilenet_v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of transfer learning is to train a classifier not to on the original images but to on the representations (__features__) obtained from the truncated network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an intermediary feature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fist approach is to first transform our image dataset into a feature dataset by applying the truncated network to all the images.\n",
    "\n",
    "We preprocess all the hard disc and RAM images using the truncated MobileNetV2. We obtain a dataset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = glob.glob('./data/hd/hd_*.jpg')\n",
    "w = 224\n",
    "img_list_hd = []\n",
    "X_list_hd = []\n",
    "for img_path in image_list:\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=(w, w))\n",
    "\n",
    "        X = image.img_to_array(img).reshape((1,w,w,3))\n",
    "        img_list_hd.append(X.copy()/255.0)\n",
    "\n",
    "        X = mobilenet_v2.preprocess_input(X)\n",
    "\n",
    "        F = model_mobilenet_no_top.predict(X)\n",
    "        X_list_hd.append(F)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "imgs_hd = np.concatenate(img_list_hd, axis=0)\n",
    "X_hd = np.concatenate(X_list_hd, axis=0)\n",
    "y_hd = np.zeros(X_hd.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = glob.glob('./data/ram/ram_*.jpg')\n",
    "\n",
    "img_list_ram = []\n",
    "X_list_ram = []\n",
    "for img_path in image_list:\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=(w, w))\n",
    "\n",
    "        X = image.img_to_array(img).reshape((1,w,w,3))\n",
    "        img_list_ram.append(X.copy()/255.0)\n",
    "\n",
    "        X = mobilenet_v2.preprocess_input(X)\n",
    "\n",
    "        F = model_mobilenet_no_top.predict(X)\n",
    "        X_list_ram.append(F)\n",
    "    except OSError:\n",
    "        pass\n",
    "imgs_ram = np.concatenate(img_list_ram, axis=0)\n",
    "X_ram = np.concatenate(X_list_ram, axis=0)\n",
    "y_ram = np.ones(X_ram.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.concatenate((imgs_hd, imgs_ram), axis=0)\n",
    "X = np.concatenate((X_hd, X_ram), axis=0)\n",
    "y = np.concatenate((y_hd, y_ram), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the features dataset into training and testing part.\n",
    "<br>\n",
    "(We do it manually so that we can keep track of the corresponding original images.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a permutation of the indicies\n",
    "number_samples = X.shape[0]\n",
    "shuffle_index = np.random.permutation(number_samples)\n",
    "\n",
    "# use the permuted list as indices\n",
    "imgs = imgs[shuffle_index, :,:,:]\n",
    "X = X[shuffle_index,:,:,:]\n",
    "y = y[shuffle_index]\n",
    "\n",
    "# Split the data in training and testing\n",
    "testing_training_ratio = 0.5\n",
    "test_samples = int(testing_training_ratio * number_samples)\n",
    "\n",
    "# from 0 to test_samples-1\n",
    "imgs_test = imgs[:test_samples]\n",
    "X_test = X[:test_samples]\n",
    "y_test = y[:test_samples]\n",
    "\n",
    "# From test_samples to end\n",
    "imgs_train = imgs[test_samples:]\n",
    "X_train = X[test_samples:]\n",
    "y_train = y[test_samples:]\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a network on the features\n",
    "\n",
    "We now train a small neural network to recognize hard discs and ram based on the features.\n",
    "\n",
    "We will use a 3 layer fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "input_shape = X[0].shape\n",
    "feature_input = Input(shape=input_shape, name=\"feature_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = Flatten()(feature_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = Dense(64, activation='relu', name='fc1')(fl)\n",
    "dp1 = Dropout(0.1)(fc1)\n",
    "fc2 = Dense(64, activation='relu', name='fc2')(dp1)\n",
    "dp2 = Dropout(0.1)(fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc3 = Dense(2, activation='softmax', name='fc3')(dp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature = Model(feature_input, fc3, name='hd_or_ram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train.\n",
    "<br>\n",
    "(Note: we use the _Adagrad_ optimizer, as it worked better then Adam for this problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_categorical(y_train)\n",
    "\n",
    "model_feature.compile(loss='binary_crossentropy', optimizer=Adagrad(lr=0.01), metrics=['acc'])\n",
    "\n",
    "model_checkpoint_cb = ModelCheckpoint(\"model_feature_weights.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=10, verbose=0, mode='auto')\n",
    "H = model_feature.fit(X_train, y_train_one_hot, batch_size=16, epochs=50, validation_split=0.25 , shuffle=True, callbacks=[model_checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'], label=\"loss\")\n",
    "plt.plot(H.history['val_loss'], label=\"val_loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"loss vs epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['acc'], label=\"acc\")\n",
    "plt.plot(H.history['val_acc'], label=\"val_acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.title(\"Accuracy vs epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "Let's reload the best model and test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature.load_weights(\"model_feature_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_one_hot = model_feature.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(y_test_pred_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "CM = confusion_matrix(y_test, y_test_pred)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.sum(np.diag(CM)) / np.sum(CM)\n",
    "print(\"Accuracy = {:.02f}\".format(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.diag(CM) / np.sum(CM, axis = 0)\n",
    "R = np.diag(CM) / np.sum(CM, axis = 1)\n",
    "for i in range(2):\n",
    "    print(\"Class '{}' : P = {:.02f} R = {:.02f}\".format(i, P[i], R[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot a few examples of correct classifications. and all the incorrect classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices = np.where(y_test_pred == y_test)[0]\n",
    "for i in correct_indices[:10]:\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs_test[i, :, :, :])\n",
    "    plt.axis('off')\n",
    "    if y_test[i] == 0:\n",
    "        title = \"True HD Pred HD\"\n",
    "    else:\n",
    "        title = \"True RAM Pred RAM\"\n",
    "    plt.title(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_indices = np.where(y_test_pred != y_test)[0]\n",
    "for i in error_indices:\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs_test[i, :, :, :])\n",
    "    plt.axis('off')\n",
    "    if y_test[i] == 0:\n",
    "        title = \"True HD Pred RAM\"\n",
    "    else:\n",
    "        title = \"True RAM Pred HD\"\n",
    "    plt.title(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it\n",
    "Use transfer learning to create a classifier for two (or more) classes that are not in the `1000` default classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_system_practice",
   "language": "python",
   "name": "info_system_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
