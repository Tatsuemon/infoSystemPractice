{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session2 - playground2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole training from the beginning / 初めからのトレーニング\n",
    "\n",
    "Let's first run the whole network training process from the start.\n",
    "\n",
    "まずは、最初からネットワークトレーニングを行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDBB; padding: 10px;\">\n",
    "<b>REMINDER</b>: If you are getting strange errors when executing code with neural networks, make sure that you stopped or restarted the kernels in all other notebooks!\n",
    "    \n",
    "<b>注意</b>：ニューラルネットワークのコードを実行しているときに見知らぬエラーが発生した場合は、他のすべてのノートブックでカーネルを停止または再起動したかを確認してください！\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Tensorflow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Import Keras stuff\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load MNIST and create the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_loader import MNISTVectorLoader\n",
    "mnist_vector_loader = MNISTVectorLoader(43)\n",
    "X, y = mnist_vector_loader.samples(70000)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the network\n",
    "<a id='define_the_network'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape\n",
    "vector_input = Input(shape = input_shape, name='input')\n",
    "\n",
    "fc1 = Dense(128, activation='relu', name='fc1')(vector_input)\n",
    "\n",
    "fc2 = Dense(128, activation='relu', name='fc2')(fc1)\n",
    "\n",
    "output = Dense(10, activation='softmax', name='output')(fc2)\n",
    "\n",
    "network = Model(vector_input, output, name='classification')\n",
    "\n",
    "network.summary()\n",
    "\n",
    "network.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = network.fit(X_train, y_train_one_hot,\n",
    "                batch_size=100, epochs=20,\n",
    "                validation_data=(X_test, y_test_one_hot),\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "Plot the evolution of the losses and accuracies.\n",
    "\n",
    "損失と精度の進化をプロットしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "Re-run the training but this time:\n",
    "- Stop the training when `val_acc` the accuracy on the validation set does not grow anymore \n",
    "- Save the model with the best accuracy as `my_best_network.hdf5`\n",
    "\n",
    "トレーニングを再実行してください。今回は\n",
    " - テストセットの精度（`val_acc`）が向上しなくなったら、トレーニングを中断する\n",
    " - モデルを `my_best_network.hdf5`として最高の精度で保存します\n",
    "\n",
    "Note that you have to recreate the network to re-initialize the weights if you want to re-train from scratch.\n",
    "<br>\n",
    "Otherwise the training just updates the current weights.\n",
    "\n",
    "<br>So, you have to re-run the cells from [Define the network](#define_the_network) to create a new model with fresh weights.\n",
    "\n",
    "注：最初からトレーニングをやり直す場合は、重みを再初期化する必要があります（そうしなければ、トレーニングは現在の重みを更新するだけです）。そのためネットワークを再作成しなければなりません。\n",
    "<br>\n",
    " [Define the network](#define_the_network)からのセルを再実行して、新しい重みを持つ新しいモデルを作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot again the evolution of the losses and accuracies.\n",
    "\n",
    "もう一度損失と精度の進化をプロットしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3 - Recognize your own writting / 自分の手書き数字の認識\n",
    "\n",
    "In the tests above the testing data was also from MNIST.\n",
    "<br>\n",
    "Let us now try to recognize some handwritten digits that are not from MNIST.\n",
    "\n",
    "今までは、トレーニングにもテストにもMNISTのデータを使っていました。\n",
    "<br>\n",
    "今度は自分が書いた手書きの数字を認識してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the network you want to test.\n",
    "\n",
    "まず試してみたいネットワークをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = load_model(\"my_best_network.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the next cell  to draw a digit to test the classifier:\n",
    "- Draw in the middle of the dark area\n",
    "- The prediction is updated at each stroke\n",
    "- Use the clear button to erase\n",
    "\n",
    "Try the classification multiple times to empirically check the accuracy of the classifier.\n",
    "\n",
    "次に、次のセルを使って数字を描いて分類器をテストしてください。\n",
    " - 暗い領域の真ん中に描いて\n",
    " - 予測は各ストロークで更新されます\n",
    " - 消去するにはクリアボタンを使用してください\n",
    "\n",
    "分類器の精度を経験的に確認するために、分類を複数回試してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import jupyter_drawing_pad as jd\n",
    "    \n",
    "jdp = jd.CustomBox()\n",
    "draw_pad = jdp.drawing_pad\n",
    "clear_btn = jdp.children[1].children[1]\n",
    "\n",
    "out = widgets.Output(layout=widgets.Layout(width='400px'))\n",
    "\n",
    "@out.capture() \n",
    "def w_CB(change):\n",
    "    from scipy.signal import convolve2d\n",
    "    from cv2 import resize, INTER_CUBIC, cvtColor, COLOR_RGB2GRAY\n",
    "\n",
    "    data = change['new']\n",
    "    if len(data[0]) > 2:\n",
    "        # Get strokes information\n",
    "        x = np.array(data[0])\n",
    "        y = np.array(data[1])\n",
    "        t = np.array(data[2])\n",
    "\n",
    "        # assuming there is at least 200ms between each stroke \n",
    "        line_breaks = np.where(np.diff(t) > 200)[0]\n",
    "        # adding end of array\n",
    "        line_breaks = np.append(line_breaks, t.shape[0])\n",
    "        \n",
    "        # Plot to canvas\n",
    "        from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "        fig = plt.figure()\n",
    "        canvas = FigureCanvas(fig)\n",
    "        ax = fig.gca()\n",
    "\n",
    "        # plot all strokes\n",
    "        plt.plot(x[:line_breaks[0]], y[:line_breaks[0]], color='black', linewidth=4)\n",
    "        for i in range(1, len(line_breaks)):\n",
    "            plt.plot(x[line_breaks[i-1]+1 : line_breaks[i]], y[line_breaks[i-1]+1 : line_breaks[i]], color='black', linewidth=4)\n",
    "        \n",
    "        plt.xlim(0,460)\n",
    "        plt.ylim(0,250)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        canvas.draw()       # draw the canvas, cache the renderer\n",
    "\n",
    "        # convert to numpy array \n",
    "        imageflat = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "        # not sure why this size...\n",
    "        image = np.reshape(imageflat,(288, 432, 3))\n",
    "        \n",
    "        # Cut the part containing the writting\n",
    "        ind = np.where(image<255)      \n",
    "        \n",
    "        D0 = ind[0].max() - ind[0].min() \n",
    "        D1 = ind[1].max() - ind[1].min() \n",
    "        \n",
    "        C0 = int(0.5 * (ind[0].max() + ind[0].min()))\n",
    "        C1 = int(0.5 * (ind[1].max() + ind[1].min()))\n",
    "\n",
    "        if D0 > D1:\n",
    "            D = D0\n",
    "        else:\n",
    "            D = D1\n",
    "        \n",
    "        L = int(D / 2.0) + 20\n",
    "        image = image[C0 - L : C0 + L ,  C1 - L : C1 + L, :] \n",
    "    \n",
    "        # Convert to gray\n",
    "        image = 255 - cvtColor(image, COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Low pass filter and resize\n",
    "        k = 12\n",
    "        I = convolve2d(image, np.ones((k,k))/k**2.0, mode=\"same\")      \n",
    "                \n",
    "        # Resize with opencv \n",
    "        I = resize(I, dsize=(28, 28), interpolation=INTER_CUBIC)\n",
    "        \n",
    "        # Clip in [0, 1]\n",
    "        I = I / I.max()\n",
    "        I = I * 3.0\n",
    "        I = I.clip(0, 1)\n",
    "        \n",
    "        # Get a feature vector\n",
    "        Xv =  I.reshape((1, 28*28)).astype(np.float64) \n",
    "        \n",
    "        # Standardization\n",
    "        Xv = scaler.transform(Xv)\n",
    "        \n",
    "        # Apply the classifier\n",
    "        y_pred_one_hot = network.predict(Xv)\n",
    "        y_prediction = np.argmax(y_pred_one_hot)\n",
    "        v = np.max(y_pred_one_hot)    \n",
    "        \n",
    "        title = \"Prediction: {} ({:.02f})\".format(y_prediction, v)    \n",
    "        \n",
    "        # draw the converted image\n",
    "        plt.clf()\n",
    "        plt.imshow(I, aspect='equal', cmap = mpl.cm.binary, interpolation='none')\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "              \n",
    "        plt.show()\n",
    "\n",
    "        # To erase after tracing\n",
    "        #change['owner'].data = [[], [], []]\n",
    "        \n",
    "        # Schedule for clearing\n",
    "        out.clear_output(wait=True)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "draw_pad.observe(w_CB, names='data')\n",
    "\n",
    "hb = widgets.HBox([draw_pad, clear_btn, out])\n",
    "display(hb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL 1\n",
    "Until now we split the dataset into testing and training sets.\n",
    "<br>\n",
    "However, in many cases, the dataset is split between a __development set__ and a testing set.\n",
    "\n",
    "For example, in a \"competition\", the competing teams only have access to the development set to build their classifier and the testing set is kept secret by the organizers.\n",
    "<br>\n",
    "After the development period is over, the organizers use the testing set to compute the accuracy of the classifiers built by the participants to decide the winner.\n",
    "\n",
    "これまでは、データセットをテストセットとトレーニングセットに分割しました。\n",
    "<br>\n",
    "ただし、多くの場合、データセットは__開発用のセット__とテストセットに分割されています。\n",
    "\n",
    "> Even if you have access to the full data including the training dataset, it is standard practice to create a test set at the begining, put it aside, and never use it. You only use it at the very end, after you decided which classifier to use and tuned it, to get the final evaluation of the performance.\n",
    "\n",
    "Since we do not have access to the testing set, the development set is usually split in two sets:\n",
    "- a training set to train the classifier\n",
    "- a __validation set__ to check if the training is well done (validate the training).\n",
    "\n",
    "In particular, the validation set is used to stop the training before overfitting occurs (early stopping).\n",
    "\n",
    "The `fit` function provides a way to automatically create a validation set:\n",
    "- the parameter `validation_split` determine the ratio of the training data to be used for validation\n",
    "- if the parameter `shuffle` is `True` a different random split is done at each epoch\n",
    "\n",
    "The next command applies `fit` with 50% of validation data and a random shuffling at each epoch:\n",
    "\n",
    "```H = network.fit(X_train, y_train_one_hot, \n",
    "                    batch_size=100, epochs=10,\n",
    "                    validation_split=0.5 , shuffle=True,\n",
    "                    callbacks=[early_stopping_cb, \n",
    "                    model_checkpoint_cb],\n",
    "                    verbose=1)\n",
    "                    ```\n",
    "                    \n",
    "This call to `fit` also sets an `EarlyStopping` callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 3 networks using:\n",
    "- 5% of the training set for validation; save the best network as `best_network_05.hdf5`\n",
    "- 50% of the training set for validation; save the best network as `best_network_50.hdf5` \n",
    "- 95% of the training set for validation; save the best network as `best_network_95.hdf5` \n",
    "\n",
    "Check the confusion matrix on the testing set for each of these three networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
